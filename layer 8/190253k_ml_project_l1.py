# -*- coding: utf-8 -*-
"""190253K_ml_project-l1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ROHCUKno0V364X2YO9l4Se3U-NVrRKeQ
"""

# import libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, mean_squared_error
from sklearn.model_selection import RandomizedSearchCV
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# read the test and train data files
train_df = pd.read_csv("train.csv")
valid_df = pd.read_csv("valid.csv")
test_df = pd.read_csv("test.csv")

def to_csv(predictions_old, predictions_new, reduced_X_train, label_no):
    data = []
    cols = ["Predicted labels before feature engineering", "Predicted labels after feature engineering", "No of new features"]

    for index in range(1, 257):
        cols.append(f"new_feature_{index}")

    for index, pred in enumerate(predictions_old):
        data.append([predictions_old[index], predictions_new[index]])

    final_no_of_features = reduced_X_train.shape[1]
    for index, row in enumerate(data):
        data[index].append(final_no_of_features)
        if index < len(reduced_X_train):
            data[index] = np.concatenate((data[index], reduced_X_train[index]))

    blank_array = np.empty((1, (256 - final_no_of_features)))
    blank_array.fill(np.nan)
    for index,row in enumerate(data):
        data[index] = np.concatenate((data[index], blank_array[0]))

    data_frame = pd.DataFrame(data, columns=cols)
    data_frame.to_csv(f"190253K_{label_no}.csv",na_rep='')

"""Lable 1"""

train_1_df = train_df.iloc[:,:-3]
valid_1_df = valid_df.iloc[:, :-3]
test_1_df = test_df.iloc[:, 1:]

train_1_df.isna().sum()

valid_1_df.isna().sum()

# splitting the test and train datasets into X and Y values
X_1_train= train_1_df.iloc[:,0:-1].values
Y_1_train = train_1_df.iloc[:,-1].values
X_1_valid = valid_1_df.iloc[:,0:-1].values
Y_1_valid = valid_1_df.iloc[:,-1].values
X_1_test = test_1_df.iloc[:,:].values

# scalling and fitting data
scaler = StandardScaler()
scaler.fit(X_1_train)

X_1_train = scaler.transform(X_1_train)
X_1_valid = scaler.transform(X_1_valid)
X_1_test = scaler.transform(X_1_test)

# compare models using cross validation
models = [RandomForestClassifier(), SVC(kernel='linear'), KNeighborsClassifier(n_neighbors=5)]

for model in models:
    cv_score = cross_val_score(model, X_1_train, Y_1_train, cv=5)
    mean_accuracy = round((sum(cv_score)/len(cv_score))*100,2)
    print("Mean accuracy % of the model: ", model, mean_accuracy)

# Use SVC since it has the highest accuracy
model = SVC(kernel='linear')
model.fit(X_1_train, Y_1_train)

y_1_valid_pred = model.predict(X_1_valid)
y_1_test_pred = model.predict(X_1_test)
print(classification_report(Y_1_valid, y_1_valid_pred))

# Create a SelectKBest instance with f_classif scoring function and select top 2 features
k = 500
selector = SelectKBest(score_func=f_classif, k=k)

# Fit and transform the data
X_1_train = selector.fit_transform(X_1_train, Y_1_train)
X_1_valid = selector.transform(X_1_valid)
X_1_test = selector.transform(X_1_test)

X_1_train.shape

model.fit(X_1_train, Y_1_train)
y_1_pred_after = model.predict(X_1_valid)
print(classification_report(Y_1_valid, y_1_pred_after))

pca=PCA(0.85)
pca = pca.fit(X_1_train)

x_1_train_pca=pca.fit_transform(X_1_train)
x_1_valid_pca = pca.transform(X_1_valid)
x_1_test_pca = pca.transform(X_1_test)

# Use SVC
model.fit(x_1_train_pca, Y_1_train)

y_1_pred_after = model.predict(x_1_valid_pca)
print(classification_report(Y_1_valid, y_1_pred_after))

x_1_train_pca.shape

param_grid = {
    'C': [0.1, 1, 10],          # Regularization parameter
    'kernel': ['linear', 'rbf'],  # Kernel type
    'gamma': ['scale', 'auto']   # Kernel coefficient for 'rbf' kernel
}

grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')

# Fit the GridSearchCV instance to the training data
grid_search.fit(x_1_train_pca, Y_1_train)

# Get the best hyperparameters and model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_1_pred_after = best_model.predict(x_1_valid_pca)

print("Best Hyperparameters:", best_params)
print(classification_report(Y_1_valid, y_1_pred_after))

preds = best_model.predict(x_1_test_pca)

data_frame = pd.DataFrame(preds, columns=["label_1"])
data_frame.to_csv(f"190253K_1.csv",na_rep='')

