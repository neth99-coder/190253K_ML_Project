# -*- coding: utf-8 -*-
"""190253K_ml_project-l4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/158bvum6Ej__XIyMpHbtvxM2xQmgjbAID
"""

# import libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, mean_squared_error
from sklearn.model_selection import RandomizedSearchCV
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# read the test and train data files
train_df = pd.read_csv("train.csv")
valid_df = pd.read_csv("valid.csv")
test_df = pd.read_csv("test.csv")

"""Label 4"""

train_4_df = train_df.iloc[:, :]
valid_4_df = valid_df.iloc[:, :]
test_4_df = test_df.iloc[:, 1:]

train_4_df.drop(columns=["label_1", "label_2", "label_3"], inplace=True)
valid_4_df.drop(columns=["label_1", "label_2", "label_3"], inplace=True)

train_4_df.isna().sum()

valid_4_df.isna().sum()

# splitting the test and train datasets into X and Y values
X_4_train= train_4_df.iloc[:,0:-1].values
Y_4_train = train_4_df.iloc[:,-1].values
X_4_valid = valid_4_df.iloc[:,0:-1].values
Y_4_valid = valid_4_df.iloc[:,-1].values
X_4_test = test_4_df.iloc[:,:].values

# scalling and fitting data
scaler = StandardScaler()
scaler.fit(X_4_train)

X_4_train = scaler.transform(X_4_train)
X_4_valid = scaler.transform(X_4_valid)
X_4_test = scaler.transform(X_4_test)

# compare models using cross validation
models = [RandomForestClassifier(), SVC(kernel='linear'), KNeighborsClassifier(n_neighbors=5)]

for model in models:
    cv_score = cross_val_score(model, X_4_train, Y_4_train, cv=5)
    mean_accuracy = round((sum(cv_score)/len(cv_score))*100,2)
    print("Mean accuracy % of the model: ", model, mean_accuracy)

# Use SVC since it has the highest accuracy
model = LogisticRegression()
model.fit(X_4_train, Y_4_train)

y_4_valid_pred = model.predict(X_4_valid)
y_4_test_pred = model.predict(X_4_test)
print(classification_report(Y_4_valid, y_4_valid_pred))

train_4_df['label_4'].value_counts().plot(kind='bar',title='Imbalanced data')

!pip install -U imbalanced-learn

from imblearn.combine import SMOTETomek

# resampling the data

resampler = SMOTETomek(random_state=0)
X_4_train, Y_4_train = resampler.fit_resample(X_4_train, Y_4_train)

# Create a SelectKBest instance with f_classif scoring function and select top 2 features
k = 500
selector = SelectKBest(score_func=f_classif, k=k)

# Fit and transform the data
X_4_train = selector.fit_transform(X_4_train, Y_4_train)
X_4_valid = selector.transform(X_4_valid)
X_4_test = selector.transform(X_4_test)

X_4_train.shape

model.fit(X_4_train, Y_4_train)
y_4_pred_after = model.predict(X_4_valid)
print(classification_report(Y_4_valid, y_4_pred_after))

preds_after_kbest = model.predict(X_4_test)

# pca=PCA(0.95)
# pca = pca.fit(X_4_train)

# x_4_train_pca=pca.fit_transform(X_4_train)
# x_4_valid_pca = pca.transform(X_4_valid)
# x_4_test_pca = pca.transform(X_4_test)

# # Use RandomForestClassifier
# model.fit(x_4_train_pca, Y_4_train)

# y_4_pred_after = model.predict(x_4_valid_pca)
# print(classification_report(Y_4_valid, y_4_pred_after))

# x_4_train_pca.shape

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10],     # Regularization parameter
    'penalty': ['l1', 'l2'],           # Regularization penalty ('l1' for L1 regularization, 'l2' for L2 regularization)
    'solver': ['liblinear', 'saga'],   # Algorithm to use in the optimization problem
}

# Create a GridSearchCV object with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')

# Fit the GridSearchCV instance to the training data
grid_search.fit(X_4_train, Y_4_train)

# # Get the best hyperparameters and model
# best_params = grid_search.best_params_
# best_model = grid_search.best_estimator_

# # Evaluate the best model on the test set
# y_4_pred_after = best_model.predict(X_4_valid)
# preds = best_model.predict(X_4_test)

# print("Best Hyperparameters:", best_params)
# print(classification_report(Y_4_valid, y_4_pred_after))

data_frame = pd.DataFrame(preds_after_kbest, columns=["label_4"])
data_frame.to_csv(f"190253K_4.csv",na_rep='')

